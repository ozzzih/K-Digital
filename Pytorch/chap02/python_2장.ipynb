{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "099ffddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "##2.2.1 텐서 다루기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "824d62aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce RTX 2070'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "750ebe80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA 사용 가능 여부 : True\n",
      "현재 사용 device : cuda:0\n",
      "CUDA Index : 0\n",
      "GPU 이름 : NVIDIA GeForce RTX 2070\n",
      "GPU 개수 : 1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "device = torch.device('cuda:0' if USE_CUDA else 'cpu')\n",
    "\n",
    "print('CUDA 사용 가능 여부 :', USE_CUDA)\n",
    "print('현재 사용 device :', device)\n",
    "print('CUDA Index :', torch.cuda.current_device())\n",
    "print('GPU 이름 :', torch.cuda.get_device_name())\n",
    "print('GPU 개수 :', torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88712725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4]])\n",
      "------------------------\n",
      "tensor([[1, 2],\n",
      "        [3, 4]], device='cuda:0')\n",
      "------------------------\n",
      "tensor([[1., 2.],\n",
      "        [3., 4.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.tensor([[1,2],[3,4]]))\n",
    "print('------------------------')\n",
    "print(torch.tensor([[1,2],[3,4]], device=\"cuda:0\")) #GPU가 없다면 오류가 발생하므로 주석 처리하였습니다.\n",
    "print('------------------------')\n",
    "print(torch.tensor([[1,2],[3,4]], dtype=torch.float64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13955c29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2]\n",
      " [3 4]]\n",
      "------------------------\n",
      "cuda:0\n",
      "[[1 2]\n",
      " [3 4]]\n"
     ]
    }
   ],
   "source": [
    "temp = torch.tensor([[1,2],[3,4]])\n",
    "print(temp.numpy())\n",
    "print('------------------------')\n",
    "\n",
    "temp = torch.tensor([[1,2],[3,4]], device=\"cuda:0\") #GPU가 없다면 오류가 발생하므로 주석 처리하였습니다.\n",
    "print(temp.device)\n",
    "#temp = torch.tensor([[1,2],[3,4]], device=\"cpu:0\")\n",
    "print(temp.to(\"cpu\").numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ceaaed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.) tensor(2.) tensor(7.)\n",
      "------------------------\n",
      "tensor([3., 4., 5.]) tensor([5., 6.])\n"
     ]
    }
   ],
   "source": [
    "temp = torch.FloatTensor([1, 2, 3, 4, 5, 6, 7])\n",
    "print(temp[0], temp[1], temp[-1])\n",
    "print('------------------------')\n",
    "print(temp[2:5], temp[4:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "35d9751c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "v = torch.tensor([1, 2, 3])\n",
    "w = torch.tensor([3, 4, 6])\n",
    "print(w - v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "50d75b43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2])\n",
      "------------------------\n",
      "tensor([[1],\n",
      "        [2],\n",
      "        [3],\n",
      "        [4]])\n",
      "------------------------\n",
      "tensor([1, 2, 3, 4])\n",
      "------------------------\n",
      "tensor([[1, 2, 3, 4]])\n",
      "------------------------\n",
      "tensor([[1],\n",
      "        [2],\n",
      "        [3],\n",
      "        [4]])\n"
     ]
    }
   ],
   "source": [
    "temp = torch.tensor([\n",
    "    [1, 2], [3, 4]\n",
    "])\n",
    "\n",
    "print(temp.shape)\n",
    "print('------------------------')\n",
    "print(temp.view(4,1))\n",
    "print('------------------------')\n",
    "print(temp.view(-1))\n",
    "print('------------------------')\n",
    "print(temp.view(1, -1))\n",
    "print('------------------------')\n",
    "print(temp.view(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42e98bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.4 파이토치 코드 맛보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2ba331fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "\n",
    "mnist_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (1.0),)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d05e6ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from six.moves import urllib    \n",
    "opener = urllib.request.build_opener()\n",
    "opener.addheaders = [('User-agent', 'Mozilla/5.0')]\n",
    "urllib.request.install_opener(opener)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2cc8882d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ../chap02/data/MNIST_DATASET\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9912422/9912422 [00:11<00:00, 885167.09it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../chap02/data/MNIST_DATASET\\MNIST\\raw\\train-images-idx3-ubyte.gz to ../chap02/data/MNIST_DATASET\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ../chap02/data/MNIST_DATASET\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28881/28881 [00:00<00:00, 146628.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../chap02/data/MNIST_DATASET\\MNIST\\raw\\train-labels-idx1-ubyte.gz to ../chap02/data/MNIST_DATASET\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ../chap02/data/MNIST_DATASET\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1648877/1648877 [00:02<00:00, 595475.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../chap02/data/MNIST_DATASET\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to ../chap02/data/MNIST_DATASET\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ../chap02/data/MNIST_DATASET\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4542/4542 [00:00<00:00, 910854.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../chap02/data/MNIST_DATASET\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ../chap02/data/MNIST_DATASET\\MNIST\\raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from torchvision.datasets import MNIST\n",
    "import requests\n",
    "download_root='../chap02/data/MNIST_DATASET'\n",
    "\n",
    "train_dataset=MNIST(download_root, transform=mnist_transform, train=True, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2db47dc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(tensor([1.4710]), tensor([0.2173])), (tensor([0.1412]), tensor([1.7285])), (tensor([0.1344]), tensor([2.2867])), (tensor([-1.3283]), tensor([-0.5386])), (tensor([0.8748]), tensor([-0.3786])), (tensor([-2.9165]), tensor([0.5221])), (tensor([1.0283]), tensor([0.5626])), (tensor([1.5960]), tensor([-0.9997])), (tensor([-1.4951]), tensor([0.4893])), (tensor([1.2379]), tensor([0.4651]))]\n",
      "Input: tensor([1.4710])\n",
      "Actual Output: tensor([0.2173])\n",
      "Predicted Output: tensor([-0.2049])\n",
      "Input: tensor([0.1412])\n",
      "Actual Output: tensor([1.7285])\n",
      "Predicted Output: tensor([0.0911])\n",
      "Input: tensor([0.1344])\n",
      "Actual Output: tensor([2.2867])\n",
      "Predicted Output: tensor([0.0922])\n",
      "Input: tensor([-1.3283])\n",
      "Actual Output: tensor([-0.5386])\n",
      "Predicted Output: tensor([-0.0317])\n",
      "Input: tensor([0.8748])\n",
      "Actual Output: tensor([-0.3786])\n",
      "Predicted Output: tensor([-0.0864])\n",
      "Input: tensor([-2.9165])\n",
      "Actual Output: tensor([0.5221])\n",
      "Predicted Output: tensor([-0.5570])\n",
      "Input: tensor([1.0283])\n",
      "Actual Output: tensor([0.5626])\n",
      "Predicted Output: tensor([-0.1216])\n",
      "Input: tensor([1.5960])\n",
      "Actual Output: tensor([-0.9997])\n",
      "Predicted Output: tensor([-0.2278])\n",
      "Input: tensor([-1.4951])\n",
      "Actual Output: tensor([0.4893])\n",
      "Predicted Output: tensor([-0.0869])\n",
      "Input: tensor([1.2379])\n",
      "Actual Output: tensor([0.4651])\n",
      "Predicted Output: tensor([-0.1623])\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import optimizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import SGD, lr_scheduler\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 데이터셋과 데이터로더를 정의해야 합니다.\n",
    "# 예를 들어, 단순한 데이터셋을 만들고 데이터로더를 생성할 수 있습니다.\n",
    "class SimpleDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "# 예제 데이터를 생성합니다.\n",
    "data = [(torch.randn(1), torch.randn(1)) for _ in range(100)]\n",
    "# 데이터셋을 생성합니다.\n",
    "dataset = SimpleDataset(data)\n",
    "# 데이터로더를 생성합니다.\n",
    "dataloader = DataLoader(dataset, batch_size=10, shuffle=True)\n",
    "\n",
    "# 모델을 생성합니다.\n",
    "def MLP(in_features=1, hidden_features=20, out_features=1):\n",
    "    hidden = nn.Linear(in_features=in_features, out_features=hidden_features, bias=True)\n",
    "    activation = nn.ReLU()\n",
    "    output = nn.Linear(in_features=hidden_features, out_features=out_features, bias=True)\n",
    "    net = nn.Sequential(hidden, activation, output)\n",
    "    return net\n",
    "\n",
    "# 모델을 생성합니다.\n",
    "model = MLP()\n",
    "\n",
    "# 손실 함수를 정의합니다.\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# 옵티마이저를 정의합니다.\n",
    "optimizer = SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "# 스케줄러를 정의합니다.\n",
    "scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda epoch: 0.95**epoch)\n",
    "\n",
    "# 모델 훈련 교재 57페이지\n",
    "for epoch in range(1, 100+1):\n",
    "    for x, y in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(model(x), y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    scheduler.step()\n",
    "\n",
    "# 학습 후의 모델을 사용하여 예측을 수행할 수 있습니다.\n",
    "###\n",
    "# 예측을 수행할 테스트 데이터를 정의합니다.\n",
    "test_data = [(torch.randn(1), torch.randn(1)) for _ in range(10)]\n",
    "print(test_data)\n",
    "# 모델을 평가 모드로 설정합니다.\n",
    "model.eval()\n",
    "\n",
    "# 각 테스트 데이터에 대한 예측을 수행합니다.\n",
    "with torch.no_grad():  # 그라디언트 계산 비활성화\n",
    "    for x_test, y_test in test_data:\n",
    "        # 입력 데이터를 모델에 전달하여 예측을 수행합니다.\n",
    "        predicted_output = model(x_test)\n",
    "        print(\"Input:\", x_test)\n",
    "        print(\"Actual Output:\", y_test)\n",
    "        print(\"Predicted Output:\", predicted_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44961535",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fb364a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(r'../chap02/data/car_evaluation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63ea085b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>maint</th>\n",
       "      <th>doors</th>\n",
       "      <th>persons</th>\n",
       "      <th>lug_capacity</th>\n",
       "      <th>safety</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>vhigh</td>\n",
       "      <td>vhigh</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>small</td>\n",
       "      <td>low</td>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>vhigh</td>\n",
       "      <td>vhigh</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>small</td>\n",
       "      <td>med</td>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>vhigh</td>\n",
       "      <td>vhigh</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>small</td>\n",
       "      <td>high</td>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>vhigh</td>\n",
       "      <td>vhigh</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>med</td>\n",
       "      <td>low</td>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>vhigh</td>\n",
       "      <td>vhigh</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>med</td>\n",
       "      <td>med</td>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   price  maint doors persons lug_capacity safety output\n",
       "0  vhigh  vhigh     2       2        small    low  unacc\n",
       "1  vhigh  vhigh     2       2        small    med  unacc\n",
       "2  vhigh  vhigh     2       2        small   high  unacc\n",
       "3  vhigh  vhigh     2       2          med    low  unacc\n",
       "4  vhigh  vhigh     2       2          med    med  unacc"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1854b091",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "fig_size = plt.rcParams[\"figure.figsize\"]\n",
    "fig_size[0] = 8\n",
    "fig_size[1] = 6\n",
    "plt.rcParams[\"figure.figsize\"] = fig_size\n",
    "dataset.output.value_counts().plot(kind='pie', autopct='%0.05f%%', colors=['lightblue', 'lightgreen', 'orange', 'pink'], explode=(0.05, 0.05, 0.05,0.05))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73c82b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = ['price', 'maint', 'doors', 'persons', 'lug_capacity', 'safety']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "281fc4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "for category in categorical_columns:\n",
    "    dataset[category] = dataset[category].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6b735e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3, 3, 0, 0, 2, 1],\n",
       "       [3, 3, 0, 0, 2, 2],\n",
       "       [3, 3, 0, 0, 2, 0],\n",
       "       [3, 3, 0, 0, 1, 1],\n",
       "       [3, 3, 0, 0, 1, 2],\n",
       "       [3, 3, 0, 0, 1, 0],\n",
       "       [3, 3, 0, 0, 0, 1],\n",
       "       [3, 3, 0, 0, 0, 2],\n",
       "       [3, 3, 0, 0, 0, 0],\n",
       "       [3, 3, 0, 1, 2, 1]], dtype=int8)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "price = dataset['price'].cat.codes.values\n",
    "maint = dataset['maint'].cat.codes.values\n",
    "doors = dataset['doors'].cat.codes.values\n",
    "persons = dataset['persons'].cat.codes.values\n",
    "lug_capacity = dataset['lug_capacity'].cat.codes.values\n",
    "safety = dataset['safety'].cat.codes.values\n",
    "\n",
    "categorical_data = np.stack([price, maint, doors, persons, lug_capacity, safety], 1)\n",
    "categorical_data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11ea6d73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3, 3, 0, 0, 2, 1],\n",
       "        [3, 3, 0, 0, 2, 2],\n",
       "        [3, 3, 0, 0, 2, 0],\n",
       "        [3, 3, 0, 0, 1, 1],\n",
       "        [3, 3, 0, 0, 1, 2],\n",
       "        [3, 3, 0, 0, 1, 0],\n",
       "        [3, 3, 0, 0, 0, 1],\n",
       "        [3, 3, 0, 0, 0, 2],\n",
       "        [3, 3, 0, 0, 0, 0],\n",
       "        [3, 3, 0, 1, 2, 1]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorical_data = torch.tensor(categorical_data, dtype=torch.int64)\n",
    "categorical_data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e765bcab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1728, 6])\n",
      "torch.Size([6912])\n"
     ]
    }
   ],
   "source": [
    "outputs = pd.get_dummies(dataset.output)\n",
    "outputs = outputs.values\n",
    "outputs = torch.tensor(outputs).flatten()\n",
    "\n",
    "print(categorical_data.shape)\n",
    "print(outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "828e0711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(4, 2), (4, 2), (4, 2), (3, 2), (3, 2), (3, 2)]\n"
     ]
    }
   ],
   "source": [
    "categorical_column_sizes = [len(dataset[column].cat.categories) for column in categorical_columns]\n",
    "categorical_embedding_sizes = [(col_size, min(50, (col_size+1)//2)) for col_size in categorical_column_sizes]\n",
    "print(categorical_embedding_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de38aa0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_records = 1728\n",
    "test_records = int(total_records * .2)\n",
    "\n",
    "categorical_train_data = categorical_data[:total_records-test_records]\n",
    "categorical_test_data = categorical_data[total_records-test_records:total_records]\n",
    "train_outputs = outputs[:total_records-test_records]\n",
    "test_outputs = outputs[total_records-test_records:total_records]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "46eb162b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1383\n",
      "1383\n",
      "345\n",
      "345\n"
     ]
    }
   ],
   "source": [
    "print(len(categorical_train_data))\n",
    "print(len(train_outputs))\n",
    "print(len(categorical_test_data))\n",
    "print(len(test_outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2eaa67ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, embedding_size, output_size, layers, p=0.4):\n",
    "        super().__init__()\n",
    "        self.all_embeddings = nn.ModuleList([nn.Embedding(ni, nf) for ni, nf in embedding_size])\n",
    "        self.embedding_dropout = nn.Dropout(p)\n",
    "        \n",
    "        all_layers = []\n",
    "        num_categorical_cols = sum((nf for ni, nf in embedding_size))\n",
    "        input_size = num_categorical_cols \n",
    "\n",
    "        for i in layers:\n",
    "            all_layers.append(nn.Linear(input_size, i))\n",
    "            all_layers.append(nn.ReLU(inplace=True))\n",
    "            all_layers.append(nn.BatchNorm1d(i))\n",
    "            all_layers.append(nn.Dropout(p))\n",
    "            input_size = i\n",
    "\n",
    "        all_layers.append(nn.Linear(layers[-1], output_size))\n",
    "        self.layers = nn.Sequential(*all_layers)\n",
    "\n",
    "    def forward(self, x_categorical):\n",
    "        embeddings = []\n",
    "        for i,e in enumerate(self.all_embeddings):\n",
    "            embeddings.append(e(x_categorical[:,i]))\n",
    "        x = torch.cat(embeddings, 1)\n",
    "        x = self.embedding_dropout(x)\n",
    "        x = self.layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4909ff0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(\n",
      "  (all_embeddings): ModuleList(\n",
      "    (0): Embedding(4, 2)\n",
      "    (1): Embedding(4, 2)\n",
      "    (2): Embedding(4, 2)\n",
      "    (3): Embedding(3, 2)\n",
      "    (4): Embedding(3, 2)\n",
      "    (5): Embedding(3, 2)\n",
      "  )\n",
      "  (embedding_dropout): Dropout(p=0.4, inplace=False)\n",
      "  (layers): Sequential(\n",
      "    (0): Linear(in_features=12, out_features=200, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): Dropout(p=0.4, inplace=False)\n",
      "    (4): Linear(in_features=200, out_features=100, bias=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): Dropout(p=0.4, inplace=False)\n",
      "    (8): Linear(in_features=100, out_features=50, bias=True)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (11): Dropout(p=0.4, inplace=False)\n",
      "    (12): Linear(in_features=50, out_features=4, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-14-2841aec1b83c>:9: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  num_categorical_cols = sum((nf for ni, nf in embedding_size))\n"
     ]
    }
   ],
   "source": [
    "model = Model(categorical_embedding_sizes, 4, [200,100,50], p=0.4)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "301d59a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3d5c313f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "56492e5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:   1 loss: 1.65249419\n",
      "epoch:  26 loss: 1.45879722\n",
      "epoch:  51 loss: 1.33384025\n",
      "epoch:  76 loss: 1.23400390\n",
      "epoch: 101 loss: 1.11513293\n",
      "epoch: 126 loss: 0.95979440\n",
      "epoch: 151 loss: 0.86270565\n",
      "epoch: 176 loss: 0.76592660\n",
      "epoch: 201 loss: 0.72367680\n",
      "epoch: 226 loss: 0.66824156\n",
      "epoch: 251 loss: 0.64819992\n",
      "epoch: 276 loss: 0.63562006\n",
      "epoch: 301 loss: 0.61229289\n",
      "epoch: 326 loss: 0.60510421\n",
      "epoch: 351 loss: 0.59476370\n",
      "epoch: 376 loss: 0.59021413\n",
      "epoch: 401 loss: 0.59014261\n",
      "epoch: 426 loss: 0.58325481\n",
      "epoch: 451 loss: 0.57738525\n",
      "epoch: 476 loss: 0.57660651\n",
      "epoch: 500 loss: 0.5747064352\n"
     ]
    }
   ],
   "source": [
    "epochs = 500\n",
    "aggregated_losses = []\n",
    "train_outputs = train_outputs.to(device=device, dtype=torch.int64)\n",
    "for i in range(epochs):\n",
    "    i += 1\n",
    "    y_pred = model(categorical_train_data)\n",
    "    single_loss = loss_function(y_pred, train_outputs)\n",
    "    aggregated_losses.append(single_loss)\n",
    "\n",
    "    if i%25 == 1:\n",
    "        print(f'epoch: {i:3} loss: {single_loss.item():10.8f}')\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    single_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(f'epoch: {i:3} loss: {single_loss.item():10.10f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fd2f1035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.56376094\n"
     ]
    }
   ],
   "source": [
    "test_outputs = test_outputs.to(device=device, dtype=torch.int64)\n",
    "with torch.no_grad():\n",
    "    y_val = model(categorical_test_data)\n",
    "    loss = loss_function(y_val, test_outputs)\n",
    "print(f'Loss: {loss:.8f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6735f451",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.6323,  0.8207, -2.8484, -2.7129],\n",
      "        [ 3.1852,  1.6122, -4.3075, -4.0866],\n",
      "        [ 2.1316,  1.0418, -3.4468, -3.0402],\n",
      "        [ 1.8252,  0.8358, -3.0042, -2.6456],\n",
      "        [ 2.1291,  1.2548, -2.8223, -2.7871]])\n"
     ]
    }
   ],
   "source": [
    "print(y_val[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5acf59f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "y_val = np.argmax(y_val, axis=1)\n",
    "print(y_val[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a7e16502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[259   0]\n",
      " [ 85   1]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      1.00      0.86       259\n",
      "           1       1.00      0.01      0.02        86\n",
      "\n",
      "    accuracy                           0.75       345\n",
      "   macro avg       0.88      0.51      0.44       345\n",
      "weighted avg       0.81      0.75      0.65       345\n",
      "\n",
      "0.7536231884057971\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "print(confusion_matrix(test_outputs,y_val))\n",
    "print(classification_report(test_outputs,y_val))\n",
    "print(accuracy_score(test_outputs, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23a4969",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
